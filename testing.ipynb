{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Asus\\.conda\\envs\\persona\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 4/4 [00:01<00:00,  2.20it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.40s/it]\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "The new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "from inference import load_fine_tuned_model, inference\n",
    "import argparse, os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "model_id = os.getenv('MODEL_ID')\n",
    "model, tokenizer = load_fine_tuned_model(model_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a crypto market expert that gives informative answers to user questions based on the context. You can be wrong but you have to be decisive<|im_end|>\n",
      "<|im_start|>user\n",
      "Answer the user question based on provided context in your way.\n",
      "            \n",
      "            Context: []\n",
      "            \n",
      "            User Question:\n",
      "            sol or eth or what? \n",
      "            \n",
      "            **NOTE**:\n",
      "              - Strictly follow the context and answer the user question.\n",
      "              - If the context is not enough to answer the question, \n",
      "            <|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "Model Response:\n",
      "\n",
      "\n",
      "\n",
      "solana terminal looks clean\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description=\"Generate responses using a fine-tuned language model.\")\n",
    "# parser.add_argument('-i', '--input', type=str, required=True, help=\"Input text for the model to process.\")\n",
    "# args = parser.parse_args()\n",
    "user_input = \"sol or eth or what?\"\n",
    "\n",
    "response = inference(model, tokenizer, user_input)\n",
    "print(\"\\nModel Response:\")\n",
    "print(response)\n",
    "#solana looks mispriced 11m too tight not worth it\n",
    "\n",
    "# after                                  \n",
    "#frame rate dropping like a rock formation trading\n",
    "#eth sol both\n",
    "# looks like eth sol market caps are getting drained waiting for something better than these token bags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
